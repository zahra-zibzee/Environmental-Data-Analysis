---
title: "LiDAR Data Processing and Tree Detection"
author: "Kasra Eskandarizanjani and Zahra hashemi"
format: 
    html:
        page-layout: full
execute: 
  freeze: auto
---

<style>
    main .content {
    max-width: 1400px;
    width: 100%;
    margin: 0 auto;
}
</style>

# Introduction
In this project, we are going to work with preprocessed LiDAR point cloud data of 10 ground plots (all same size) of a forest in Russia. The goal is to identify the trees in the plots. Our group has chosen task no. 4, which first, we are going to create depth images out of point cloud datasets for each plot, segmenting the trees based on the ground truth we have for individual trees, then, we are going to apply Unet or ResNet to identify those individual trees in the depth images. 


In the cell below, you can see a visualisation of plot 1.

```{python}
import os
import numpy as np
import pandas as pd
import laspy
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import rasterio
from rasterio.plot import show
import cv2
import json
import glob

from utils import lidar_to_point_cloud, tree_scope_definition


def visualize_lidar(lidar_file: str | os.PathLike):
    """Visualize a LiDAR point cloud file (.las or .laz)."""
    points = lidar_to_point_cloud(lidar_file)
    print(points.shape)
    fig = plt.figure(figsize=(10, 10))
    ax = fig.add_subplot(111, projection="3d")
    ax.scatter(
        points[:, 0], points[:, 1], points[:, 2], c=points[:, 2], cmap="viridis", s=0.5
    )
    ax.set_title("LiDAR Point Cloud Visualization")
    ax.set_xlabel("X")
    ax.set_ylabel("Y")
    ax.set_zlabel("Z")
    plt.show()


visualize_lidar("data/als/plot_01.las")
```


# Data Preprocessing

here, a histogram of the Z values of the point cloud is plotted. The red dashed lines represent the first and third quartiles plus/minus 1.5 times the IQR.

```{python}
points_01 = lidar_to_point_cloud("data/als/plot_01.las")
IQRz = np.percentile(points_01[:, 2], 75) - np.percentile(points_01[:, 2], 25)
plt.hist(points_01[:, 2], bins=100, color="blue", alpha=0.7)
plt.axvline(np.percentile(points_01[:, 2], 25) - 1.5 * IQRz, color="red", linestyle="--")
plt.axvline(np.percentile(points_01[:, 2], 75) + 1.5 * IQRz, color="red", linestyle="--")
plt.title("Histogram of Z Values (Plot 01)")
plt.xlabel("Z Value")
plt.ylabel("Frequency")
plt.show()
```


## General Method to Create Depth Images

Converting a point cloud (shape `(218000, 3)`, representing 3D points (x, y, z)) into a 2D depth image involves projecting the 3D points onto a 2D plane and encoding the depth information (usually the z-coordinate or distance from the camera).

Here’s how you can do it:

1. **Define the camera projection:**
   - Choose a camera's intrinsic parameters (focal length, principal point, etc.).
   - Set the resolution of the depth image (e.g., 640 \times 480).

2. **Project points to the 2D plane:**
   - Convert (x, y, z) into image coordinates (u, v) using:
     $$
     u = f_x \cdot \frac{x}{z} + c_x, \quad v = f_y \cdot \frac{y}{z} + c_y
     $$
     where f_x, f_y are the focal lengths (in pixels), and c_x, c_y are the principal points (image center).

3. **Create a depth map:**
   - Map each 3D point to its corresponding (u, v) pixel in the 2D image.
   - Store the z-value (or -z for convention) in the depth map at (u, v).
   - Handle occlusions by keeping the smallest z-value for each (u, v) to ensure only the closest point is recorded.

4. **Normalize depth values:**
   - Scale the depth values to a range suitable for visualization (e.g., 0–255 for 8-bit images).

---

## Depth Image Creation

But we don't have the camera intrinsic parameters. Since we are going to map from the top, we will use this method:

Group by x and y, and get the maximum z value. This will give us the depth map. Then we will normalize the z-values to 0-255, to get the grayscale image.

For better results, we remove the height values that are outliers. We use the IQR method to remove the outliers. 

```{python}
points = pd.DataFrame(lidar_to_point_cloud("data/als/plot_01.las"), columns=["x", "y", "z"])
points["x"] = points["x"].astype(int)
points["y"] = points["y"].astype(int)
depth_map = points.groupby(["x", "y"])["z"].max().reset_index()
Q1 = np.percentile(depth_map["z"], 25)
Q3 = np.percentile(depth_map["z"], 75)
IQR = Q3 - Q1
plt.hist(depth_map["z"], bins=100, color="b", alpha=0.7)
plt.axvline(Q1 - 1.5 * IQR, color="r", linestyle="--")
plt.axvline(Q3 + 1.5 * IQR, color="r", linestyle="--")
plt.title("Histogram of Depth Map")
plt.xlabel("Depth Value")
plt.ylabel("Frequency")
plt.show()
```

here is a representation of the depth map of plot 1.

```{python}
# Remove outliers
depth_map = depth_map[
    (depth_map["z"] >= Q1 - 1.5 * IQR) & (depth_map["z"] <= Q3 + 1.5 * IQR)
]

# convert the point cloud to a depth map
depth_map["x"] = (depth_map["x"] - depth_map["x"].min()).astype(int)
depth_map["y"] = (depth_map["y"] - depth_map["y"].min()).astype(int)
img = np.zeros((depth_map["x"].max() + 1, depth_map["y"].max() + 1)) + depth_map["z"].min()
print(img.size, img.shape)
print(depth_map.shape)
img[depth_map["x"], depth_map["y"]] = depth_map["z"]
img = (img / img.max() * 255).astype(np.uint8)

# histogram equalization
img = cv2.equalizeHist(img)

print(img, img.dtype)
# plt.hist(img.flatten(), bins=255, range=(0, 255), color="b", alpha=0.7)


plt.figure(figsize=(10, 10))
plt.imshow(img, cmap="gray")
plt.axis("off")
plt.title("Depth Map")
plt.show()
```


Now lets put everything in a function and use step as a parameter to group the points.

The above cells represent the steps we plan to take. The taken steps are outlined in the following implemented functions.

In the `tree_scope_definition` function, based on the location of individual trees in the ground truth geojson file, we label the points in the depth image that are within the circle of the tree's location and diameter. The function returns the depth image with the labeled points.

```{python}
from utils import lidar_to_point_cloud, tree_scope_definition

plot_num = 1
raw_depth_map = pd.DataFrame(
    lidar_to_point_cloud(f"data/als/plot_{plot_num:02d}.las"), columns=["x", "y", "z"]
)
gt_depth_map = tree_scope_definition(raw_depth_map, 1)
print(gt_depth_map["label"].value_counts().sort_index())
# plt.hist(gt_depth_map["label"], bins=5, color="b", alpha=0.7)
gt_depth_map
```

We previously converted the x and y coordinates of the point cloud to integer values for simplicity. However, in this step, we use the original float values and define a step size to group points by their x and y coordinates. In this way, we increase the resolution and precision of the depth image.

```{python}
from utils import take_photo_from_top


plot_num = 1
raw_depth_map = pd.DataFrame(
    lidar_to_point_cloud(f"data/als/plot_{plot_num:02d}.las"), columns=["x", "y", "z"]
)
gt_depth_map = tree_scope_definition(raw_depth_map, 1)
depth_map = take_photo_from_top(gt_depth_map, step=0.1)


plt.hist(depth_map["z"], bins=100, color="b", alpha=0.7)
plt.title("Histogram of Depth Map")
plt.xlabel("Depth Value")
plt.ylabel("Frequency")
plt.show()
```


```{python}
from utils import create_depth_img


raw_depth_map = pd.DataFrame(
    lidar_to_point_cloud("data/als/plot_01.las"), columns=["x", "y", "z"]
)
gt_depth_map = tree_scope_definition(raw_depth_map, 1)
depth_map = take_photo_from_top(gt_depth_map)
# print(depth_map.columns)
depth_img, mask, missing_pixels = create_depth_img(depth_map)

depth_img_3d = cv2.cvtColor(depth_img, cv2.COLOR_GRAY2RGB)
depth_img_3d[mask == 1] = [255, 0, 0]
# depth_img_3d[missing_pixels == 1] = [0, 0, 255]

plt.figure(figsize=(10, 10))
plt.imshow(depth_img_3d)
plt.axis("off")
plt.title("Depth Map")
plt.show()
```

The mask image is showing the identified tree points in the depth image. 

```{python}
plt.figure(figsize=(10, 10))
plt.imshow(mask, cmap="gray")
plt.axis("off")
plt.title("Mask")
plt.show()
```

## Plot Rotation (Data Preprocessing)

When we examine the original plots, we notice that the rectangles have been rotated. To align the images correctly, we will take the following steps:

### Calculating the Rotation Angle
1. **Sort Points**: 
    - First, we need to sort the points by their y-coordinates to distinguish the top two points and the bottom two points.
    - Then, we did sort the top two points by their x-coordinates.

2. **Calculate the Angle**:
    - Using the `arctan2` function, the angle between the top points has been calculated:
    
    $$ 
    \text{atan2}(y, x) = 
    \begin{cases} 
    \arctan\left(\frac{y}{x}\right) & \text{if } x > 0, \\[10pt]
    \arctan\left(\frac{y}{x}\right) + \pi & \text{if } x < 0 \text{ and } y \geq 0, \\[10pt]
    \arctan\left(\frac{y}{x}\right) - \pi & \text{if } x < 0 \text{ and } y < 0, \\[10pt]
    +\frac{\pi}{2} & \text{if } x = 0 \text{ and } y > 0, \\[10pt]
    -\frac{\pi}{2} & \text{if } x = 0 \text{ and } y < 0, \\[10pt]
    \text{undefined} & \text{if } x = 0 \text{ and } y = 0.
    \end{cases}
    $$

### Rotating the Points
1. **Define the Rotation Matrix**:
    - Using the calculated angle \(\theta\) to define the rotation matrix:

    $$
    R = 
    \begin{bmatrix}
    \cos(\theta) & -\sin(\theta) \\
    \sin(\theta) & \cos(\theta)
    \end{bmatrix}
    $$

2. **Rotate the Points**:
    - By applying the rotation matrix to the points, we will get the rotated points:
    
    ```python
    rotated_points = points @ rotation_matrix
    ```
    

3. **Adjust the Orientation**:
    - Based on the shape (landscape or portrait), we can adjust the rotation as well.





```{python}

from utils import calculate_rotation_angle, turn_points

```

## Create The Depth Images for all the Data Grounds

```{python}
from utils import create_dataset

all_depth_imgs, all_masks, all_missing_pixels, angles = create_dataset(
    "data/als", orientation=False
)
```

## Visualize the Depth Images - No Rotation

```{python}
fig, axes = plt.subplots(5, 2, figsize=(10, 25))
axes = axes.flatten()
if len(all_depth_imgs) > len(axes):
    indices = np.random.choice(range(len(all_depth_imgs)), len(axes), replace=False)
else:
    indices = range(len(all_depth_imgs))

for i, ax in zip(indices, axes):
    depth_img, mask, missing_pixel = (
        all_depth_imgs[i],
        all_masks[i],
        all_missing_pixels[i],
    )
    depth_img_3d = cv2.cvtColor(depth_img, cv2.COLOR_GRAY2RGB)
    depth_img_3d[mask == 1] = [255, 0, 0]
    # depth_img_3d[missing_pixel == 1] = [0, 0, 255]

    ax.imshow(depth_img_3d)
    ax.axis("off")
    ax.set_title(f"Plot {i + 1}, Angle: {angles[i]:.2f}°")
plt.tight_layout()
plt.show()
```

## Visualize the Depth Images - With Rotation (Landscape)

```{python}
all_depth_imgs, all_masks, all_missing_pixels, angles = create_dataset(
    "data/als", orientation="landscape")
fig, axes = plt.subplots(5, 2, figsize=(10, 25))
axes = axes.flatten()
if len(all_depth_imgs) > len(axes):
    indices = np.random.choice(range(len(all_depth_imgs)), len(axes), replace=False)
else:
    indices = range(len(all_depth_imgs))

for i, ax in zip(indices, axes):
    depth_img, mask, missing_pixel = (
        all_depth_imgs[i],
        all_masks[i],
        all_missing_pixels[i],
    )
    depth_img_3d = cv2.cvtColor(depth_img, cv2.COLOR_GRAY2RGB)
    depth_img_3d[mask == 1] = [255, 0, 0]
    # depth_img_3d[missing_pixel == 1] = [0, 0, 255]or ax, (img, mask) in zip(axes, val_loader):

    ax.imshow(depth_img_3d)
    ax.axis("off")
    ax.set_title(f"Plot {i + 1}, Angle: {angles[i]:.2f}°")
plt.tight_layout()
plt.show()
```

Let's check the images sizes:
```{python}
for img in all_depth_imgs:
    print(img.shape)
``` 

These images are too big, let's split each image into several patches and use them as the input of the model.
To make sure to keep the trees in the patches, we will use overlapping patches. 

You can run `python utils.py` to create the patches and save them in the `data/patches` folder.

# Model design and training process

## Dataset

The dataset class is availabe in the `model_training.py` file. The dataset class is called `ImagesDataset` and it is used to load the images and masks from the folder where the patches are saved.


> To Have better result in the model, the dataset recieves a parameter named `extend_radius` which is used to extend the mask of the trees in the images. This is done to make sure the trees are big enough in the images to be detected by the model. The default value for this parameter is 20 pixels.
```{python}
from model_training import ImagesDataset

dataset = ImagesDataset("data/patches")

fig, axes = plt.subplots(3, 2, figsize=(10, 5))
axes = axes.flatten()
for ax, (img, mask) in zip(axes, dataset):
    print(img.shape, mask.shape)
    assert (
        mask.shape[-1] == 2
    ), f"The mask should have 2 channels, {mask.shape[0]} found."
    mask = mask.argmax(-1)
    img = np.stack([img, img, img], axis=-1)
    img[mask == 1] = [255, 0, 0]
    ax.imshow(img)
    ax.axis("off")
plt.tight_layout()
plt.show()
```

## Model Architecture - Unet

>**Note:** A pretrained model was not utilized in this project because the images in our dataset differ significantly from the domain of images on which the pretrained model was originally trained.

## Model Architecture

We took the Unet model from [this repo](https://github.com/milesial/Pytorch-UNet/tree/master/unet) and we will use it to train our model.
Here we can see the model architecture of the Unet model we are going to use.
```{python}
from model_training import setup_model

model, optimizer, criterion, device = setup_model()

print(model)
```


## Data Loading and Training

```{python}
# | eval: false
from model_training import train_model


all_train_loss, all_val_loss, model, val_loader, train_loader = train_model(
    data_folder="./data/patches",
    model_save_path="unet_model.pth",
    epochs=10,
    batch_size=16,
    learning_rate=1e-3,
)
```

```{python}
# | echo: false
print("""
ython model_training.py 
100%|████████████████████████████████████████████████████████| 115/115 [00:39<00:00,  2.90it/s]
Epoch 1/10, Train Loss: 0.1579, Val Loss: 0.0677
100%|████████████████████████████████████████████████████████| 115/115 [00:35<00:00,  3.24it/s]
Epoch 2/10, Train Loss: 0.0556, Val Loss: 0.0483
100%|████████████████████████████████████████████████████████| 115/115 [00:35<00:00,  3.25it/s]
Epoch 3/10, Train Loss: 0.0459, Val Loss: 0.0435
100%|████████████████████████████████████████████████████████| 115/115 [00:35<00:00,  3.26it/s]
Epoch 4/10, Train Loss: 0.0431, Val Loss: 0.0421
100%|████████████████████████████████████████████████████████| 115/115 [00:35<00:00,  3.28it/s]
Epoch 5/10, Train Loss: 0.0416, Val Loss: 0.0410
100%|████████████████████████████████████████████████████████| 115/115 [00:35<00:00,  3.28it/s]
Epoch 6/10, Train Loss: 0.0409, Val Loss: 0.0405
100%|████████████████████████████████████████████████████████| 115/115 [00:34<00:00,  3.29it/s]
Epoch 7/10, Train Loss: 0.0404, Val Loss: 0.0401
100%|████████████████████████████████████████████████████████| 115/115 [00:35<00:00,  3.27it/s]
Epoch 8/10, Train Loss: 0.0400, Val Loss: 0.0397
100%|████████████████████████████████████████████████████████| 115/115 [00:35<00:00,  3.27it/s]
Epoch 9/10, Train Loss: 0.0395, Val Loss: 0.0408
100%|████████████████████████████████████████████████████████| 115/115 [00:34<00:00,  3.29it/s]
Epoch 10/10, Train Loss: 0.0390, Val Loss: 0.0399
Model saved to unet_model.pth
Train and validation loss saved to train_val_loss.npy
""")
```
```{python}
# | echo: false

from model_training import create_dataset

train_val_loss = np.load("train_val_loss.npy")
all_train_loss, all_val_loss = train_val_loss[0], train_val_loss[1]

train_loader, val_loader = create_dataset("data/patches", batch_size=16)
```

```{python}
plt.figure(figsize=(10, 5))
plt.plot(all_train_loss, label="Train Loss")
plt.plot(all_val_loss, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training and Validation Loss")
plt.legend()
plt.show()
```

## Model Evaluation
```{python}
# | echo: false
import torch
from model_training import UNet

model, optimizer, criterion, device = setup_model()
model.load_state_dict(
    torch.load("unet_model.pth", map_location=device, weights_only=True)
)
model.eval()
None
```

```{python}
fig, axes = plt.subplots(4, 3, figsize=(10, 10))
legend_elements = [
    plt.Line2D(
        [0],
        [0],
        marker="o",
        color="w",
        markerfacecolor="green",
        markersize=10,
        label="Predicted",
    ),
    plt.Line2D(
        [0],
        [0],
        marker="o",
        color="w",
        markerfacecolor="red",
        markersize=10,
        label="Ground Truth",
    ),
]

axes[0, -1].legend(
    handles=legend_elements, loc="center left", bbox_to_anchor=(1.1, 0.5)
)

axes = axes.flatten()

for ax, (img, mask) in zip(axes, val_loader):
    img = img.to(device)
    mask = mask.to(device)
    pred = model(img).detach().cpu()
    img = img[0].squeeze(0).cpu().numpy()
    img = (img - img.min()) / (img.max() - img.min()) * 255
    mask = mask[0].squeeze(0).cpu().numpy().argmax(0)
    pred = pred[0].squeeze(0).cpu().numpy().argmax(0)
    img = np.stack([img, img, img], axis=-1).astype(np.uint8)
    img[mask == 1] = [255, 0, 0]
    img[pred == 1] = [0, 255, 0]
    ax.imshow(img)
    ax.axis("off")


plt.tight_layout()
plt.show()
```

The model outputs a segmentation map—a binary matrix of 0s and 1s with the same dimensions as the input image. In this map, pixels with a value of 1 represent tree regions, while pixels with a value of 0 represent the background. However, to determine the final locations of individual trees, further post-processing is required to identify distinct tree positions from the segmentation output.

One approach to achieve this is by using clustering algorithms. For example, **DBSCAN** is particularly suitable because it identifies clusters (tree locations) without requiring the number of clusters (trees) to be predefined. This is a significant advantage over algorithms like **k-means**, which require the number of clusters as an input—a parameter that is not known in this case, given the varying number of trees across different images.

Alternatively, **image processing techniques** can be employed to find connected components in the binary segmentation map. Connected component analysis groups neighboring pixels with a value of 1 into distinct regions, each representing an individual tree. This method is computationally efficient and well-suited for binary data like segmentation maps, making it a reliable option for detecting tree locations in this context.

Both approaches offer viable solutions, and the choice depends on factors like computational efficiency, the quality of the segmentation output, and the characteristics of the tree distribution in the images.

When deciding between clustering algorithms and image processing techniques for extracting tree locations from a segmentation map, the choice largely depends on the specific characteristics of the problem:

- **Image Processing (Connected Components Analysis):**
This method is generally better for binary segmentation maps, as it is straightforward, computationally efficient, and does not require any additional hyperparameter tuning. It directly identifies distinct connected regions of pixels with a value of 1, making it highly reliable for well-defined and non-overlapping tree regions. However, it may struggle with noisy or fragmented segmentation maps, where tree regions are not clearly separated or contain gaps between pixels. To overcome these limitations, additional preprocessing steps like morphological operations (e.g., dilation and erosion) can be applied to refine the segmentation map before extracting tree locations.

- **Clustering Algorithms (e.g., DBSCAN):**
DBSCAN is better suited when the segmentation map has noise or when tree regions are not strictly connected but rather consist of scattered pixels. It can handle irregular shapes and separate clusters based on spatial density, making it more robust in noisy conditions. However, it requires tuning hyperparameters like the neighborhood radius (eps) and the minimum number of points in a cluster.


The function is available in the `utils.py` file. The function is called `find_tree_locations`. 


1. **Morphological Preprocessing:**  
   - The segmentation map is converted to binary format if not already binary.
   - A morphological opening operation (erosion followed by dilation) removes small noise regions while preserving the shape of larger tree regions. The kernel size determines the level of noise removal.

2. **Connected Components Analysis:**  
   - After cleaning, the connected components are labeled uniquely.
   - The centroid of each labeled region is calculated using the `scipy.ndimage.center_of_mass` function, which identifies the approximate location of each tree.

for instance, in the latest segmentation we had:
```{python}
from utils import find_tree_locations

plt.imshow(img)
plt.axis("off")
plt.title("Segmentation Map")
plt.show()

trees = find_tree_locations(pred.astype(np.uint8))
# print(
#     f"Dectected trees are:",
#     *[f"Tree {i + 1: >2d}: ({x:6.2f}, {y:6.2f})" for i, (x, y) in enumerate(trees)],
#     sep="\n",
# )
pd.DataFrame(
    trees, columns=["X", "Y"], index=[f"Tree {i + 1}" for i in range(len(trees))]
)
```

```{python}
gt_trees = find_tree_locations(mask.astype(np.uint8))
pd.DataFrame(
    gt_trees, columns=["X", "Y"], index=[f"Tree {i + 1}" for i in range(len(gt_trees))]
)
```


## Performance evaluation and insights


In this stage of the project, we address a critical challenge in tree detection: the number of ground truth trees (manually labeled) and predicted trees (from the detection model) may not be the same. Moreover, there is no predefined one-to-one correspondence between the two sets, making direct comparisons infeasible. To evaluate the accuracy of the detection model and understand the spatial relationships between ground truth and predicted trees, a systematic method is needed to compare their positions. 

### Pairwise Distance Calculation

The `compute_pairwise_distances` function solves this problem by calculating the pairwise Euclidean distances between the two sets of tree coordinates. This function produces a distance matrix, where each entry represents the spatial distance between a predicted tree and a ground truth tree. The matrix enables flexible matching and evaluation strategies, such as identifying the closest pairs or analyzing unmatched trees, even when the number of trees differs between the two sets. 

The function relies on the `distance.cdist` method from the `scipy.spatial` module. This method efficiently computes all pairwise Euclidean distances between two sets of points. For each pair of points $(x_1, y_1)$ and $(x_2, y_2)$, it calculates the distance as:

$$
d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
$$

The resulting matrix has dimensions $M \times N$, where $M$ is the number of predicted trees and $N$ is the number of ground truth trees. This distance matrix provides the foundation for evaluating and refining the tree detection model's performance.

### Evaluation with Threshold-Based Matching

The `evaluate_with_threshold` assesses the performance of the detection model by comparing predicted and ground truth tree coordinates within a specified distance threshold. A prediction is considered correct if it is within the threshold distance of any ground truth tree.

1. **Distance Matrix Calculation**: The function computes the pairwise distance matrix using the `compute_pairwise_distances` function.
2. **Matching Predictions**: For each predicted tree, the minimum distance to any ground truth tree is calculated. It will be considered as a match if this distance is below the threshold.
3. **Metrics Calculation**:
   - **True Positives (TP)**: Count of predictions matched to ground truth within the threshold.
   - **False Positives (FP)**: Predicted trees that are unmatched.
   - **False Negatives (FN)**: Ground truth trees that are unmatched.
   - **Precision, Recall, and F1-Score** are calculated using the TP, FP, and FN values.


### Hungarian Algorithm for Optimal Matching

The `evaluate_with_hungarian` function uses the Hungarian algorithm to find an optimal one-to-one correspondence between predicted and ground truth trees. This method minimizes the total distance between matched pairs and it ensures the most accurate evaluation possible.

1. **Optimal Assignment**: Using the distance matrix, the Hungarian algorithm identifies the best matching pairs of predicted and ground truth trees.
2. **Threshold Filtering**: Matched pairs exceeding the threshold distance are excluded. We only consider pairs with accurate predictions.
3. **Metrics Calculation**:
   - **True Positives (TP)**: Matched pairs within the threshold.
   - **False Positives (FP)**: Predicted trees that remain unmatched after filtering.
   - **False Negatives (FN)**: Ground truth trees that remain unmatched.


### Error Metrics: RMSE and MAE

To quantify the positional accuracy of matched pairs, we compute **Root Mean Squared Error (RMSE)** and **Mean Absolute Error (MAE)**. These metrics provide a smooth evaluation of the spatial accuracy of predictions.

#### RMSE:
- By calculating RMSE, we emphasize on larger errors, making it sensitive to outliers in the predictions.

#### MAE:
- By calculating MAE, we provide an average of absolute positional errors, offering a more balanced view of prediction accuracy.

Both metrics rely on the optimal matching calculated by the Hungarian algorithm, to get a comprehensive and better understanding of the model's performance.

Now lets evaluate the model using the above functions.

```{python}
from eval import (
    evaluate_with_threshold,
    evaluate_with_hungarian,
    compute_rmse,
    compute_mae,
)

threshold = 10
for func in [
    evaluate_with_threshold,
    evaluate_with_hungarian,
    compute_rmse,
    compute_mae,
]:
    print(func.__name__)
    out = func(gt_trees, trees, threshold)
    if isinstance(out, dict):
        print("\n".join(f"\t{k:<10}: {v}" for k, v in out.items()))
    else:
        print(out)
    print("-----------------------------------")
```

### Evaluation over all the dataset

```{python}
from torch.utils.data import DataLoader
from collections import defaultdict


train_loader, val_loader = create_dataset("data/patches", batch_size=1)

metrics = defaultdict(list)
threshold = 10
for img, mask in val_loader:
    assert img.shape[0] == 1, "Batch size should be 1 for evaluation."
    img = img.to(device)
    mask = mask.to(device)
    pred = model(img).detach().cpu()
    mask = mask.squeeze().cpu().numpy().argmax(0)
    pred = pred.squeeze().cpu().numpy().argmax(0)
    if len(np.unique(mask)) == 1 or len(np.unique(pred)) == 1:
        continue
    gt_trees = find_tree_locations(mask.astype(np.uint8))
    trees = find_tree_locations(pred.astype(np.uint8))
    if not gt_trees.size or not trees.size:
        continue
    for func in [
        evaluate_with_threshold,
        evaluate_with_hungarian,
        compute_rmse,
        compute_mae,
    ]:
        out = func(gt_trees, trees, threshold)
        if isinstance(out, dict):
            for k, v in out.items():
                metrics[f"{func.__name__}_{k}"].append(v)
        else:
            metrics[func.__name__].append(out)

max_length = ml = max(len(k) for k in metrics.keys())
for k, v in metrics.items():
    if k.endswith("tives"):
        metrics[k] = np.mean(v)
    else:
        print(f"{k:<{ml}}: {np.mean(v):.2f}")
```

```{python}
fig, axes = plt.subplots(1, 2, figsize=(10, 10))

conf_matrix_hungarian = [
    [
        metrics["evaluate_with_hungarian_true_positives"],
        metrics["evaluate_with_hungarian_false_negatives"],
    ],
    [metrics["evaluate_with_hungarian_false_positives"], 0],
]

print(conf_matrix_hungarian)
conf_matrix_threshold = [
    [
        metrics["evaluate_with_threshold_true_positives"],
        metrics["evaluate_with_threshold_false_negatives"],
    ],
    [metrics["evaluate_with_threshold_false_positives"], 0],
]
print(conf_matrix_threshold)

axes[0].imshow(conf_matrix_hungarian, cmap="Blues")
axes[0].set_title("Hungarian Algorithm")
axes[0].set_xticks([0, 1], ["Predicted", "Unmatched"])
axes[0].set_yticks([0, 1], ["Ground Truth", "Unmatched"], rotation=90)


axes[1].imshow(conf_matrix_threshold, cmap="Blues")
axes[1].set_title("Threshold-Based Matching")
axes[1].set_xticks([0, 1], ["Predicted", "Unmatched"])
axes[1].set_yticks([0, 1], ["Ground Truth", "Unmatched"], rotation=90)

for i in range(2):
    for j in range(2):
        axes[0].text(
            j,
            i,
            f"{conf_matrix_hungarian[i][j]:.3f}",
            ha="center",
            va="center",
            color="red",
        )
        axes[1].text(
            j,
            i,
            f"{conf_matrix_threshold[i][j]:.3f}",
            ha="center",
            va="center",
            color="red",
        )

plt.show()
```


## Baseline Model (Local Maxima Filtering)

```{python}
import scipy.spatial


def local_maxima_filter(cloud: np.ndarray, window_size: float) -> np.ndarray:
    """Detect local maxima in the point cloud with a fixed window size."""

    assert isinstance(
        cloud, np.ndarray
    ), f"Cloud needs to be a numpy array, not {type(cloud)}"

    # cloud = cloud[cloud[:, 2] > height_threshold]
    tree = scipy.spatial.KDTree(data=cloud)
    seen_mask = np.zeros(cloud.shape[0], dtype=bool)
    local_maxima = []

    for i, point in enumerate(cloud):
        if seen_mask[i]:
            continue
        neighbor_indices = tree.query_ball_point(point, window_size)
        highest_neighbor = neighbor_indices[cloud[neighbor_indices, 2].argmax()]
        seen_mask[neighbor_indices] = True
        seen_mask[highest_neighbor] = False
        # This may lead to not every point being marked as seed in the end, but it does not matter,
        # because by the time the seen value is overwritten the point is already processed
        if i == highest_neighbor:
            local_maxima.append(i)

    return cloud[local_maxima]
```

Let's find the best window size for the local maxima filter.

```{python}

def read_geojason_by_plot(file_path: str | os.PathLike, plot_num: int) -> pd.DataFrame:
    with open(file_path) as f:
        data = json.load(f)
    data = pd.DataFrame([i["properties"] | i['geometry'] for i in data["features"]])
    data['x'] = data['coordinates'].apply(lambda x: x[0])
    data['y'] = data['coordinates'].apply(lambda x: x[1])
    data = data.drop(columns=['coordinates'])

    # filter data by plot number
    data = data[data["plot"] == plot_num]
    return data

def test_local_maxima(window_sizes: list):
    for plot_num in range(1, 11):
        print(f"Plot {plot_num}")
        gt_data = read_geojason_by_plot("data/field_survey.geojson", plot_num)
        points = laspy.read(f"data/als/plot_{plot_num:02d}.las")
        for window_size in window_sizes:
            lm_points = local_maxima_filter(points.xyz, window_size)
            point_distances = []
            for lm_point in lm_points:
                min_distance = np.inf
                for gt_point in gt_data[['x', 'y']].values:
                    distance = np.linalg.norm(lm_point[:2] - gt_point)
                    if distance < min_distance:
                        min_distance = distance
                point_distances.append(min_distance)
            point_distances = [float(distance) for distance in point_distances]
            print(f"Window size: {window_size}")
            # print("Sum distance:", np.sum(point_distances))
            # print("Median distance:", np.median(point_distances))
            print("local maxima filter (number of trees):", lm_points.shape[0])
            print("ground truth (number of trees):", gt_data.shape[0])
        # plt.figure(figsize=(10, 5))
        # plt.hist(point_distances, bins=20, color='skyblue', edgecolor='black')
        # plt.xlabel('Distance to nearest ground truth point')
        # plt.ylabel('Frequency')
        # plt.show()

test_local_maxima([2, 3, 4])
```